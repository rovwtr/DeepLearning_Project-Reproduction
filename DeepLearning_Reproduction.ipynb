{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hluhWS6aMQeA"
      },
      "source": [
        "#**Prepare Data & Package**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQnGu3cPL_7Q"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/facebookresearch/DeltaCNN.git\n",
        "!pip install /content/DeltaCNN\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Preprocessing data: convert to tensors and normalize by subtracting dataset\n",
        "# mean and dividing by std.\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "# Get data from torchvision.datasets\n",
        "train_data = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
        "test_data = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
        "\n",
        "#BEGIN WALGELIJK STUKJE CODE\n",
        "\n",
        "train_set = []\n",
        "for j in range(9):\n",
        "  for i in range(len(train_data)):\n",
        "    img, ground_truth = train_data[i]\n",
        "    if (ground_truth == (j+1)):\n",
        "      train_set.append((img, ground_truth))\n",
        "\n",
        "test_set = []\n",
        "for j in range(9):\n",
        "  for i in range(len(test_data)):\n",
        "    img, ground_truth = test_data[i]\n",
        "    if (ground_truth == (j+1)):\n",
        "      test_set.append((img, ground_truth))\n",
        "\n",
        "train_data = train_set\n",
        "test_data = test_set\n",
        "#EIND WALGELIJK STUKJE CODE\n",
        "\n",
        "# Define data loaders used to iterate through dataset\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=False)\n",
        "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmYPhAN_MutZ"
      },
      "source": [
        "#**Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TM3c2UegMuQd"
      },
      "outputs": [],
      "source": [
        "import deltacnn\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from torchsummary import summary\n",
        "from PIL import Image\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8N33OWGNCCE"
      },
      "source": [
        "#**Visualize Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlVzL93JNHbh"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(5, 5, figsize=(5, 5))\n",
        "for i in range(25):\n",
        "    x, _ = train_data[i]\n",
        "    ax = axs[i // 5][i % 5]\n",
        "    ax.imshow(x.view(28, 28), cmap='gray')\n",
        "    ax.axis('off')\n",
        "    ax.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9dgxvTdNNGA"
      },
      "source": [
        "#**Set-up Experiment**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULDLjJvRNMYZ"
      },
      "outputs": [],
      "source": [
        "#EVAL FUNCTION \n",
        "def evaluate_accuracy(data_loader, net, device=torch.device('cuda:0')):\n",
        "    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n",
        "    net.eval()  #make sure network is in evaluation mode\n",
        "\n",
        "    acc_sum = torch.tensor([0], dtype=torch.float32, device=device)\n",
        "    n = 0\n",
        "\n",
        "    for X, y in data_loader:\n",
        "        # Copy the data to device.\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        if (y.size(0) != 64):\n",
        "          return acc_sum.item()/n;\n",
        "          \n",
        "        with torch.no_grad():\n",
        "            y = y.long()\n",
        "            acc_sum += torch.sum((torch.argmax(net(X), dim=1) == y))\n",
        "            n += y.shape[0] #increases with the number of samples in the batch\n",
        "    return acc_sum.item()/n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OlpIC4LMQMn4"
      },
      "outputs": [],
      "source": [
        "#NET \n",
        "class Net(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_features):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, hidden_channels[0],kernel_size=3,padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.max_pool1 = nn.MaxPool2d(2)\n",
        "        self.conv2 = nn.Conv2d(hidden_channels[0], hidden_channels[1],kernel_size=5,padding=2)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.max_pool2 = nn.MaxPool2d(2)\n",
        "        self.fc = nn.Linear(7*7*hidden_channels[1], out_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.max_pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.max_pool2(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x) \n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7u588SpNgQg"
      },
      "outputs": [],
      "source": [
        "#DELTACNN NET\n",
        "import deltacnn\n",
        "class DeltaCNNNet(deltacnn.DCModule):\n",
        "    def __init__(self, in_channels, hidden_channels, out_features):\n",
        "        super(DeltaCNNNet, self).__init__()\n",
        "        self.sparsify = deltacnn.DCSparsify()\n",
        "        self.conv1 = deltacnn.DCConv2d(in_channels,hidden_channels[0], kernel_size=3, padding=1, activation=\"relu\", dense_out=False)\n",
        "        self.max_pool1 = deltacnn.DCMaxPooling(kernel_size=2, stride=2, padding=0, dilation=1)\n",
        "        self.conv2 = deltacnn.DCConv2d(hidden_channels[0], hidden_channels[1], kernel_size=5, padding=2, activation=\"relu\", dense_out=False)\n",
        "        self.max_pool2 = deltacnn.DCMaxPooling(kernel_size=2, stride=2, padding=0, dilation=1)\n",
        "        self.densify = deltacnn.DCDensify()\n",
        "        self.fc = nn.Linear(7*7*hidden_channels[1], out_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.cuda()\n",
        "        x = self.sparsify(x)\n",
        "        x = self.conv1(x)\n",
        "        x = self.max_pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.max_pool2(x)\n",
        "        x = self.densify(x)\n",
        "        x = x.reshape(x.size(0), (x.size(1)*x.size(2)*x.size(3)))\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJQg7o9vMitf"
      },
      "source": [
        "#**Define parameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PV8rKaKrMpmp"
      },
      "outputs": [],
      "source": [
        "n_runs = 25\n",
        "\n",
        "in_channels = 1 \n",
        "hidden_channels = [5, 6]\n",
        "out_features = 10 \n",
        "device = torch.device('cuda:0')\n",
        "learning_rate = 0.001\n",
        "epochs = 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JB-7OU8yQhkQ"
      },
      "source": [
        "#**Experiment**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33DCyRygQmFK"
      },
      "outputs": [],
      "source": [
        "time_lst_regular = []\n",
        "train_loss_regular = []\n",
        "train_acc_regular = []\n",
        "test_acc_regular = []\n",
        "\n",
        "for r in range(n_runs):\n",
        "  t1 = time.perf_counter()\n",
        "  ###CODE\n",
        "  print(r)\n",
        "  train_losses = []\n",
        "  train_accs = []\n",
        "  test_accs = []\n",
        "  net = Net(in_channels, hidden_channels, out_features)\n",
        "  optimizer = torch.optim.SGD(net.parameters(), lr = learning_rate)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  for epoch in range(epochs):\n",
        "      net.train()\n",
        "      net.to(device)\n",
        "      for i, (x_batch, y_batch) in enumerate(train_loader):\n",
        "          if (y_batch.size(0) != 64):\n",
        "            break\n",
        "          x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "          optimizer.zero_grad()\n",
        "          y_pred = net(x_batch)\n",
        "          loss = criterion(y_pred, y_batch)\n",
        "          train_losses.append(loss)          \n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "      train_acc = 100*evaluate_accuracy(train_loader, net.to(device))\n",
        "      test_acc = 100*evaluate_accuracy(test_loader, net.to(device))      \n",
        "      train_accs.append(train_acc)\n",
        "      test_accs.append(test_acc)\n",
        "\n",
        "      # # Print performance\n",
        "      # print('Epoch: {:.0f}'.format(epoch+1))\n",
        "      # print('Accuracy of train set: {:.00f}%'.format(train_acc))\n",
        "      # print('Accuracy of test set: {:.00f}%'.format(test_acc))\n",
        "      # print('')\n",
        "  \n",
        "  train_loss_regular.append(train_losses[-1])\n",
        "  train_acc_regular.append(train_accs[-1])\n",
        "  test_acc_regular.append(test_accs[-1])\n",
        "\n",
        "  ###CODE\n",
        "  t2 = time.perf_counter()\n",
        "  time_run = t2-t1\n",
        "  time_lst_regular.append(time_run)\n",
        "\n",
        "print(time_lst_regular)\n",
        "print(train_loss_regular)\n",
        "print(train_acc_regular)\n",
        "print(test_acc_regular)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Q8jDDoUSMIK"
      },
      "outputs": [],
      "source": [
        "time_lst_DELTA = []\n",
        "train_loss_DELTA = []\n",
        "train_acc_DELTA = []\n",
        "test_acc_DELTA = []\n",
        "\n",
        "for r in range(n_runs):\n",
        "  t1 = time.perf_counter()\n",
        "  ###CODE\n",
        "  print(r)\n",
        "  train_losses = []\n",
        "  train_accs = []\n",
        "  test_accs = []\n",
        "\n",
        "  net = DeltaCNNNet(in_channels, hidden_channels, out_features)\n",
        "  net.to(device, memory_format=torch.channels_last)\n",
        "  net.process_filters()\n",
        "  optimizer = torch.optim.SGD(net.parameters(), lr = learning_rate)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "      net.train()\n",
        "      net.to(device)\n",
        "      net = net.cuda()\n",
        "      for i, (x_batch, y_batch) in enumerate(train_loader):\n",
        "          if (y_batch.size(0) != 64):\n",
        "            break\n",
        "          x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "          optimizer.zero_grad()\n",
        "          y_pred = net(x_batch)\n",
        "          loss = criterion(y_pred, y_batch)\n",
        "          train_losses.append(loss)          \n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "      train_acc = 100*evaluate_accuracy(train_loader, net.to(device))\n",
        "      test_acc = 100*evaluate_accuracy(test_loader, net.to(device))      \n",
        "      train_accs.append(train_acc)\n",
        "      test_accs.append(test_acc)\n",
        "\n",
        "      # # Print performance\n",
        "      # print('Epoch: {:.0f}'.format(epoch+1))\n",
        "      # print('Accuracy of train set: {:.00f}%'.format(train_acc))\n",
        "      # print('Accuracy of test set: {:.00f}%'.format(test_acc))\n",
        "      # print('')\n",
        "  \n",
        "  train_loss_DELTA.append(train_losses[-1])\n",
        "  train_acc_DELTA.append(train_accs[-1])\n",
        "  test_acc_DELTA.append(test_accs[-1])\n",
        "\n",
        "  ###CODE\n",
        "  t2 = time.perf_counter()\n",
        "  time_run = t2-t1\n",
        "  time_lst_DELTA.append(time_run)\n",
        "\n",
        "print(time_lst_DELTA)\n",
        "print(train_loss_DELTA)\n",
        "print(train_acc_DELTA)\n",
        "print(test_acc_DELTA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPiirzgZXJ70"
      },
      "outputs": [],
      "source": [
        "plt.scatter(time_lst_regular, test_acc_regular, label='Regular CNN')\n",
        "plt.scatter(time_lst_DELTA, test_acc_DELTA, label= 'Delta CNN')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.xlabel('Time per run (s)')\n",
        "plt.ylabel('Test accuracy per run (%)')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#EVAL FUNCTION \n",
        "def evaluate_accuracy(data_loader, net, device=torch.device('cuda:0'), b_size=64):\n",
        "    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n",
        "    net.eval()  #make sure network is in evaluation mode\n",
        "\n",
        "    acc_sum = torch.tensor([0], dtype=torch.float32, device=device)\n",
        "    n = 0\n",
        "\n",
        "    for X, y in data_loader:\n",
        "        # Copy the data to device.\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        if (y.size(0) != b_size):\n",
        "          return acc_sum.item()/n;\n",
        "          \n",
        "        with torch.no_grad():\n",
        "            y = y.long()\n",
        "            acc_sum += torch.sum((torch.argmax(net(X), dim=1) == y))\n",
        "            n += y.shape[0] #increases with the number of samples in the batch\n",
        "    return acc_sum.item()/n\n",
        "\n",
        "n_runs = 1\n",
        "\n",
        "in_channels = 1 \n",
        "hidden_channels = [5, 6]\n",
        "out_features = 10 \n",
        "device = torch.device('cuda:0')\n",
        "learning_rate = 0.001\n",
        "epochs = 6\n",
        "\n",
        "batch_lst = [64, 64, 64, 512, 1048]\n",
        "\n",
        "b_time_lst_regular = []\n",
        "b_train_loss_regular = []\n",
        "b_train_acc_regular = []\n",
        "b_test_acc_regular = []\n",
        "\n",
        "for b_size in batch_lst:\n",
        "  train_loader = DataLoader(train_data, batch_size=b_size, shuffle=False)\n",
        "  test_loader = DataLoader(test_data, batch_size=b_size, shuffle=False)\n",
        "\n",
        "  time_lst_regular = []\n",
        "  train_loss_regular = []\n",
        "  train_acc_regular = []\n",
        "  test_acc_regular = []\n",
        "\n",
        "  for r in range(n_runs):\n",
        "    t1 = time.perf_counter()\n",
        "    ###CODE\n",
        "    print(r)\n",
        "    train_losses = []\n",
        "    train_accs = []\n",
        "    test_accs = []\n",
        "    net = Net(in_channels, hidden_channels, out_features)\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr = learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    for epoch in range(epochs):\n",
        "        net.train()\n",
        "        net.to(device)\n",
        "        for i, (x_batch, y_batch) in enumerate(train_loader):\n",
        "            if (y_batch.size(0) != b_size):\n",
        "              break\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = net(x_batch)\n",
        "            loss = criterion(y_pred, y_batch)\n",
        "            train_losses.append(loss)          \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        train_acc = 100*evaluate_accuracy(train_loader, net.to(device), b_size)\n",
        "        test_acc = 100*evaluate_accuracy(test_loader, net.to(device), b_size)      \n",
        "        train_accs.append(train_acc)\n",
        "        test_accs.append(test_acc)\n",
        "\n",
        "        # # Print performance\n",
        "        # print('Epoch: {:.0f}'.format(epoch+1))\n",
        "        # print('Accuracy of train set: {:.00f}%'.format(train_acc))\n",
        "        # print('Accuracy of test set: {:.00f}%'.format(test_acc))\n",
        "        # print('')\n",
        "    \n",
        "    train_loss_regular.append(train_losses[-1])\n",
        "    train_acc_regular.append(train_accs[-1])\n",
        "    test_acc_regular.append(test_accs[-1])\n",
        "\n",
        "    ###CODE\n",
        "    t2 = time.perf_counter()\n",
        "    time_run = t2-t1\n",
        "    time_lst_regular.append(time_run)\n",
        "\n",
        "  print(time_lst_regular)\n",
        "  print(train_loss_regular)\n",
        "  print(train_acc_regular)\n",
        "  print(test_acc_regular)\n",
        "\n",
        "  b_time_lst_regular.append(time_lst_regular[0])\n",
        "  b_train_loss_regular.append(train_loss_regular[0])\n",
        "  b_train_acc_regular.append(train_acc_regular[0])\n",
        "  b_test_acc_regular.append(test_acc_regular[0])\n",
        "\n",
        "print(b_time_lst_regular)\n",
        "print(b_train_loss_regular)\n",
        "print(b_train_acc_regular)\n",
        "print(b_test_acc_regular)\n",
        "  \n",
        "  \n",
        "\n"
      ],
      "metadata": {
        "id": "HS0g_sHXlZqe"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}